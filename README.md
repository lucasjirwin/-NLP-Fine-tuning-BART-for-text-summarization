# Fine-tuning Facebook AI's BART for Text Summarization (Junior Independent Research @ Princeton)

In this project, I set out to fine-tune and train BART on the Big Patent data set in order to improve abstractive summarization performance. The improved performance is achieved due to the structural improvements inherent in the Big Patent data set, which improves upon the weaknesses of existing text summarization data sets from the news domain by (i) ensuring that important information is dispersed throughout the document and (ii) reducing the amount of summary material repeated verbatim in predictions. I utilize Pytorch Lightning to fine-tune the model via transfer learning and evaluate the results by comparing the ROUGE scores achieved in my project to the ROUGE scores achieved by the authors of BART on previous news data sets such as the CNN/Daily Mail and XSumm data sets.

Promising results are achieved on both the category ’a’ and category ’g’ subsets of the Big Patent data set, with ROUGE-1 precision scores beating the BART model’s performance on both the CNN/Daily Mail and XSum data sets by 1-2 points and ROUGE-L scores beating the XSum data set by 2 points. The significant improvements in performance achieved in this project have proven my initial hypothesis that the structure of a data set is relevant to the performance of state-of-the-art abstractive text summarization models. The results thus inform future strategies for improving text summarization performance by shifting the focus from improving models to improving the structure of the underlying data as an alternative solution.

The improvements in abstractive summarization performance seen in this project also bring us closer to democratizing the power of natural language processing by allowing smaller companies, NGOs and local governments to use the latest, most advanced models with smaller amounts of data so as to avoid accruing exorbitant training costs.

<b> Junior_Independent_Work_Code.ipynb </b> contains the code used to fine-tune and train Facebook AI's BART.
<b> Abstractive Text Summarization with BART.pdf </b> contains the project report written in LateX for my Princeton Junior Independent Work. 
