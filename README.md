# Princeton-Junior-Independent-Work


Text summarization is perhaps the natural language processing application with the most obvious real-world applications. From law to government to finance, countless industries stand to benefit from the accurate and efficient summarization of documents. In the past 5 years, new pre-trained models such as Google’s BERT [6] (Bidirectional Encoder Representations from Transformers) and OpenAI’s GPT-3 [13] (Generative Pre-trained Transformer) have made it possible to train text summarization models on much smaller data sets by allowing users to simply fine-tune pre-trained models using transfer learning. At the same time, the release of newer, higher quality data sets such as the Big Patent data [5] set has improved the performance of the more challenging abstractive summarization task. Given these developments, my project aims to improve the performance of abstractive text summarization even further by fine-tuning Facebook AI’s BART model [4] on the new, previously unused Big Patent data set. The improvements in abstractive summarization performance seen in this project bring us closer to democratizing the power of natural language processing by allowing smaller companies, NGOs and local governments to use the latest, most advanced models with smaller amounts of data so as to avoid accruing exorbitant training costs.
